{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Word embedding is just a fancy way of saying numerical representation of words. \n",
    "\n",
    "Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in \"vector space\" (arrays) with several dimensions. Word embeddings can be generated using fancy methods like neural networks, co-occurrence matrix, probabilistic models, etc.\n",
    "\n",
    "The program \"Word2Vec\" consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer, one hidden layer and one output layer. Word2Vec utilizes two types of training sets :\n",
    "\n",
    "(1) __CBOW (Continuous Bag of Words)__\n",
    "\n",
    "> CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"images/intro-cbow.png\"></p>\n",
    "\n",
    "(2) __Skip Gram__\n",
    "\n",
    "> Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"images/intro-skipgram.png\"></p>\n",
    "\n",
    "Before we talk about these 2 Word2Vec techniques, let's go back to the basics with the samplest \"words embedding\" using what has been called \"One-hot encoding\" of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest word embedding you can have is using one-hot vectors. If you have 10,000 words in your vocabulary, then you can represent each word as a 1x10,000 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple example, if we have 4 words — mango, strawberry, city, Delhi — in our vocabulary then we can represent them as following:\n",
    "* nango [1, 0, 0, 0]\n",
    "* strawberry [0, 1, 0, 0]\n",
    "* city [0, 0, 1, 0]\n",
    "* delhi [0, 0, 0, 1]\n",
    "\n",
    "There are a few problems with the above approach, firstly, our size of vectors depends on the size of our vocabulary(which can be huge). This is a wastage of space and increases algorithm complexity exponentially resulting in the curse of dimensionality.\n",
    "\n",
    "Secondly, these embedding will be closely coupled to their applications, making transfer-learning to a model using a different vocabulary of the same size, adding/removing words from vocabulary would almost impossible as it would require to re-train the whole model again.\n",
    "\n",
    "Lastly, the entire purpose of creating embedding is to capture the contextual meaning of the words, which this representation fails to do. There is no co-relation between words that have similar meaning or usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(a1, a2):\n",
    "    return (a1 - a2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mango = np.array((1, 0, 0, 0))\n",
    "strawberry = np.array((0, 1, 0, 0))\n",
    "city = np.array((0, 0, 1, 0))\n",
    "delhi = np.array((0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity(mango, strawberry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity(mango, city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW vs Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Bag of Words Model (CBOW) and Skip-grams are techniques to represent words for training machine learning algorithms.\n",
    "\n",
    "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. While in the Skip-gram model, the distributed representation of the input word is used to predict the context. This digram illustrates this difference:\n",
    "\n",
    "<img src=\"images/cbow_skipgram.png\">\n",
    "\n",
    "Remember that both of these are simply ways to create labeled training data.  How do you a train a neural network to predict word embedding when you don’t have any labeled data i.e words and their corresponding word embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "We’ll do so by creating a “fake” task for the machine learning algorithm to train on.\n",
    "\n",
    "The fake task for Skip-gram model would be, given a word, we’ll try to predict its neighboring words. We’ll define a neighboring word by the window length — a fancy term called \"hyper-parameter\"!\n",
    "\n",
    "Here is an example with window length of 2:\n",
    "\n",
    "<img src=\"images/skipgram.png\">\n",
    "\n",
    "Given the sentence:\n",
    "\n",
    "<p style=\"text-align: center; font-style: italic;\">“I will have orange juice and eggs for breakfast.”</p>\n",
    "\n",
    "and a window length of 2, if the target word is juice, its neighboring words will be ( have, orange, and, eggs). Our input and target word pair would be (juice, have), (juice, orange), (juice, and), (juice, eggs).\n",
    "\n",
    "Also note that within the sample window, proximity of the words to the source word plays no role. So have, orange, and, and eggs will be treated the same while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import nltk    \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "# nltk.download('punkt') \n",
    "import warnings \n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handle=open(\"data/alice.txt\", \"r\")\n",
    "text=file_handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text=text.replace(\"\\n\", \"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each sentence in the file, and lower-casing all tokens\n",
    "for i in sent_tokenize(full_text): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in alphanumeric characters\n",
    "# import re\n",
    "# text = re.sub(r'\\W+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "CBOW_model = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9256836\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' and 'wonderland' - CBOW : \", \n",
    "    CBOW_model.wv.similarity('alice', 'wonderland')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9809821\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' and 'machines' - CBOW : \", \n",
    "    CBOW_model.wv.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Skip Gram model \n",
    "Skipgram_model = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.61861473\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' and 'wonderland' - Skip Gram : \", \n",
    "    Skipgram_model.wv.similarity('alice', 'wonderland')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.84913594\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' and 'machines' - Skip Gram : \", \n",
    "      Skipgram_model.wv.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output indicates the cosine similarities between word vectors ‘alice’, ‘wonderland’ and ‘machines’ for different models. One interesting task might be to change the parameter values of ‘size’ and ‘window’ to observe the variations in the cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
